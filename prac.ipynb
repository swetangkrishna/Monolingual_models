{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:38: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:38: SyntaxWarning: invalid escape sequence '\\c'\n",
      "/var/folders/2m/_9mkxd6x3_d26ct7x886r2zw0000gn/T/ipykernel_63991/3915117748.py:38: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"# Use a single, consistent definition of reference_db\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Helpers: basic text utilities\n",
    "# -----------------------------\n",
    "\n",
    "import re  # make sure this is imported somewhere\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = re.compile(r'[.!?]+')\n",
    "WORD_REGEX = re.compile(r\"\\b[\\w'-]+\\b\", re.UNICODE)\n",
    "\n",
    "# Simple list of cohesion connectors\n",
    "CONNECTORS = [\n",
    "    # Common general connectors\n",
    "    \"however\",\n",
    "    \"therefore\",\n",
    "    \"indeed\",\n",
    "    \"for this reason\",\n",
    "    \"for these reasons\",\n",
    "    \"to this extent\",\n",
    "    \"to this end\",\n",
    "    \"in addition\",\n",
    "    \"moreover\",\n",
    "    \"furthermore\",\n",
    "    \"on the other hand\",\n",
    "    \"for example\",\n",
    "    \"for instance\",\n",
    "    \"as a result\",\n",
    "    \"consequently\",\n",
    "\n",
    "    # Simple ones already covered in the base list (kept for completeness)\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"because\",\n",
    "    \"so\",\n",
    "    \"although\",\n",
    "]\n",
    "\n",
    "# Ground-truth reference metadata for your manual-search / web-scraping layer.\n",
    "\"\"\"# Use a single, consistent definition of reference_db\n",
    "reference_db = [\n",
    "    ReferenceMetadata(\n",
    "        id=\"mono-lingual\",        # matches \\cite{mono-lingual}\n",
    "        title=\"(Fill real title for mono-lingual paper here)\",\n",
    "        source_type=\"conference\",  # or \"journal\" / \"arxiv\" / \"other\"\n",
    "        year=2023,\n",
    "    ),\n",
    "    ReferenceMetadata(\n",
    "        id=\"Kang_2025\",           # matches \\cite{Kang_2025}\n",
    "        title=\"(Fill real title for Kang 2025 paper here)\",\n",
    "        source_type=\"journal\",\n",
    "        year=2025,\n",
    "    ),\n",
    "    ReferenceMetadata(\n",
    "        id=\"[1]\",\n",
    "        title=\"Attention Is All You Need\",\n",
    "        source_type=\"conference\",\n",
    "        year=2017,\n",
    "    ),\n",
    "    ReferenceMetadata(\n",
    "        id=\"Smith2020\",\n",
    "        title=\"A Journal Paper on Transformers\",\n",
    "        source_type=\"journal\",\n",
    "        year=2020,\n",
    "    ),\n",
    "]\"\"\"\n",
    "\n",
    "# Simple list of pattern phrases indicating \"quasi-definitions\"\n",
    "QUASI_DEF_PATTERNS = [\n",
    "    r\"\\bis defined as\\b\",\n",
    "    r\"\\bcan be defined as\\b\",\n",
    "    r\"\\bis called\\b\",\n",
    "    r\"\\brefers to\\b\",\n",
    "    r\"\\bis the process of\\b\",\n",
    "    r\"\\bis a type of\\b\",\n",
    "]\n",
    "\n",
    "# Simple syllable counter (heuristic)\n",
    "VOWELS = \"aeiouy\"\n",
    "\n",
    "JARGON_LIST = [\n",
    "    # General field\n",
    "    \"natural language processing\",\n",
    "    \"nlp\",\n",
    "    \"large language model\",\n",
    "    \"large language models\",\n",
    "    \"language model\",\n",
    "    \"language models\",\n",
    "    \"llm\",\n",
    "    \"llms\",\n",
    "    \"ai systems\",\n",
    "    \"artificial intelligence\",\n",
    "    \"corpora\",\n",
    "    \"corpus\",\n",
    "    \"pre-training\",\n",
    "    \"pretrained\",\n",
    "    \"pre-training corpora\",\n",
    "    \"training corpora\",\n",
    "\n",
    "    # Evaluation / benchmarking\n",
    "    \"benchmark\",\n",
    "    \"benchmarks\",\n",
    "    \"testing dataset\",\n",
    "    \"testing datasets\",\n",
    "    \"questionnaire\",\n",
    "    \"questionnaires\",\n",
    "    \"reasoning capabilities\",\n",
    "    \"reasoning tasks\",\n",
    "    \"chain-of-thought\",\n",
    "    \"cot\",\n",
    "    \"token\",\n",
    "    \"tokens\",\n",
    "    \"efficiency\",\n",
    "\n",
    "    # Multilingual / monolingual\n",
    "    \"multilingual\",\n",
    "    \"multi-lingual\",\n",
    "    \"monolingual\",\n",
    "    \"language choice\",\n",
    "    \"language-specific properties\",\n",
    "    \"lexical gap\",\n",
    "    \"lexical gaps\",\n",
    "    \"semantic distinctions\",\n",
    "\n",
    "    # Research / methodology\n",
    "    \"research questions\",\n",
    "    \"research question\",\n",
    "    \"reliability\",\n",
    "    \"model performance\",\n",
    "    \"model behaviour\",\n",
    "    \"reasoning abilities\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def count_syllables_in_word(word: str) -> int:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "    if not word:\n",
    "        return 0\n",
    "    syllables = 0\n",
    "    prev_is_vowel = False\n",
    "    for ch in word:\n",
    "        is_vowel = ch in VOWELS\n",
    "        if is_vowel and not prev_is_vowel:\n",
    "            syllables += 1\n",
    "        prev_is_vowel = is_vowel\n",
    "    # Remove silent 'e' at end if there is more than one syllable\n",
    "    if word.endswith(\"e\") and syllables > 1:\n",
    "        syllables -= 1\n",
    "    return max(syllables, 1)\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    sentences = [s.strip() for s in SENTENCE_SPLIT_REGEX.split(text) if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenize_words(text: str) -> List[str]:\n",
    "    return WORD_REGEX.findall(text)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data models\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class LinguisticClarityScores:\n",
    "    words_per_sentence: float\n",
    "    jargon_per_sentence: float\n",
    "    flesch_reading_ease: float\n",
    "    connectors_per_sentence: float\n",
    "\n",
    "@dataclass\n",
    "class ScientificAccuracyScores:\n",
    "    quasi_definitions_per_answer: int\n",
    "    bias_markers_per_answer: int\n",
    "    cosine_similarity_q_a: float\n",
    "\n",
    "@dataclass\n",
    "class ReferenceMetadata:\n",
    "    \"\"\"Ground-truth reference info from manual search / web-scraping.\"\"\"\n",
    "    id: str                 # e.g. \"[1]\" or \"Smith2020\"\n",
    "    title: str\n",
    "    source_type: str        # \"journal\", \"conference\", \"arxiv\", \"other\"\n",
    "    year: Optional[int]     # publication year\n",
    "\n",
    "@dataclass\n",
    "class ReferenceScores:\n",
    "    hallucinated_citations_per_answer: int\n",
    "    source_quality_score: float\n",
    "    recency_score: float\n",
    "\n",
    "@dataclass\n",
    "class RubricResult:\n",
    "    linguistic_clarity: LinguisticClarityScores\n",
    "    scientific_accuracy: ScientificAccuracyScores\n",
    "    references: ReferenceScores\n",
    "\n",
    "    def as_dict(self):\n",
    "        return {\n",
    "            \"linguistic_clarity\": asdict(self.linguistic_clarity),\n",
    "            \"scientific_accuracy\": asdict(self.scientific_accuracy),\n",
    "            \"references\": asdict(self.references),\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rubric evaluator\n",
    "# -----------------------------\n",
    "\n",
    "class RubricEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        jargon_list: Optional[List[str]] = None,\n",
    "        connectors: Optional[List[str]] = None,\n",
    "        reference_db: Optional[List[ReferenceMetadata]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        jargon_list: list of domain-specific technical terms (lowercase).\n",
    "        connectors: list of cohesion connectors (lowercase).\n",
    "        reference_db: ground-truth references obtained from manual search / web-scraping.\n",
    "        \"\"\"\n",
    "        self.jargon_list = set((j.lower() for j in (jargon_list or [])))\n",
    "        self.connectors = [c.lower() for c in (connectors or CONNECTORS)]\n",
    "        self.reference_db = reference_db or []\n",
    "        self.reference_ids = {ref.id for ref in self.reference_db}\n",
    "\n",
    "    # -------------------------\n",
    "    # Linguistic clarity\n",
    "    # -------------------------\n",
    "\n",
    "    def evaluate_linguistic_clarity(self, answer: str) -> LinguisticClarityScores:\n",
    "        sentences = split_sentences(answer)\n",
    "        num_sentences = max(len(sentences), 1)\n",
    "        words = tokenize_words(answer)\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Complexity: words per sentence\n",
    "        words_per_sentence = num_words / num_sentences\n",
    "\n",
    "        # Jargon words per sentence\n",
    "        jargon_count = 0\n",
    "        if self.jargon_list:\n",
    "            for w in words:\n",
    "                if w.lower() in self.jargon_list:\n",
    "                    jargon_count += 1\n",
    "        jargon_per_sentence = jargon_count / num_sentences\n",
    "\n",
    "        # Flesch Reading Ease\n",
    "        syllables = sum(count_syllables_in_word(w) for w in words) or 1\n",
    "        words_per_sentence_for_flesch = num_words / num_sentences\n",
    "        syllables_per_word = syllables / max(num_words, 1)\n",
    "        flesch = 206.835 - 1.015 * words_per_sentence_for_flesch - 84.6 * syllables_per_word\n",
    "\n",
    "        # Cohesion: average connectors per sentence\n",
    "        connectors_count = 0\n",
    "        lower_answer = answer.lower()\n",
    "        for c in self.connectors:\n",
    "            # count occurrences of connector as a word / phrase\n",
    "            connectors_count += len(re.findall(r\"\\b\" + re.escape(c) + r\"\\b\", lower_answer))\n",
    "        connectors_per_sentence = connectors_count / num_sentences\n",
    "\n",
    "        return LinguisticClarityScores(\n",
    "            words_per_sentence=words_per_sentence,\n",
    "            jargon_per_sentence=jargon_per_sentence,\n",
    "            flesch_reading_ease=flesch,\n",
    "            connectors_per_sentence=connectors_per_sentence,\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Scientific accuracy\n",
    "    # -------------------------\n",
    "\n",
    "    def _count_quasi_definitions(self, answer: str) -> int:\n",
    "        count = 0\n",
    "        lower = answer.lower()\n",
    "        for pattern in QUASI_DEF_PATTERNS:\n",
    "            count += len(re.findall(pattern, lower))\n",
    "        return count\n",
    "\n",
    "    def _count_bias_markers(self, question: str, answer: str) -> int:\n",
    "        \"\"\"\n",
    "        Heuristic: we treat 'bias' as copying loaded / subjective language\n",
    "        from the question into the answer. This is just a proxy!\n",
    "        \"\"\"\n",
    "        # Subjective / loaded words (extend as needed)\n",
    "        loaded_terms = [\n",
    "            \"obviously\", \"clearly\", \"of course\", \"everyone knows\",\n",
    "            \"always\", \"never\", \"terrible\", \"disaster\", \"perfect\",\n",
    "        ]\n",
    "\n",
    "        q_words = set(w.lower() for w in tokenize_words(question))\n",
    "        a_words = [w.lower() for w in tokenize_words(answer)]\n",
    "\n",
    "        # Count loaded terms that appear in both question and answer\n",
    "        count = 0\n",
    "        for w in a_words:\n",
    "            if w in loaded_terms and w in q_words:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def _cosine_similarity_q_a(self, question: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Accuracy via cosine similarity between TF-IDF representations of question and answer.\n",
    "        This is *semantic-ish* but not using any LLM.\n",
    "        \"\"\"\n",
    "        texts = [question, answer]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform(texts)\n",
    "        sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0, 0]\n",
    "        return float(sim)\n",
    "\n",
    "    def evaluate_scientific_accuracy(self, question: str, answer: str) -> ScientificAccuracyScores:\n",
    "        quasi_defs = self._count_quasi_definitions(answer)\n",
    "        bias_markers = self._count_bias_markers(question, answer)\n",
    "        cos_sim = self._cosine_similarity_q_a(question, answer)\n",
    "        return ScientificAccuracyScores(\n",
    "            quasi_definitions_per_answer=quasi_defs,\n",
    "            bias_markers_per_answer=bias_markers,\n",
    "            cosine_similarity_q_a=cos_sim,\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # References\n",
    "    # -------------------------\n",
    "\n",
    "    def _extract_citation_ids(self, answer: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Very simple citation patterns:\n",
    "        - [1], [2], ...\n",
    "        - (Smith, 2020) -> we map to \"Smith2020\"\n",
    "        You can extend this depending on your citation style.\n",
    "        \"\"\"\n",
    "        citation_ids = []\n",
    "\n",
    "        # Numeric style: [1], [2]\n",
    "        for cid in re.findall(r\"\\[(\\d+)\\]\", answer):\n",
    "            citation_ids.append(f\"[{cid}]\")\n",
    "\n",
    "        # Author-year: (Smith, 2020)\n",
    "        for auth, year in re.findall(r\"\\(([A-Z][A-Za-z]+),\\s*(\\d{4})\\)\", answer):\n",
    "            citation_ids.append(f\"{auth}{year}\")\n",
    "\n",
    "        return citation_ids\n",
    "\n",
    "    def _compute_source_quality_score(self, used_ids: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Journal >= Conference >= Arxiv >= Other\n",
    "        We compute an average quality score over all matched references.\n",
    "        \"\"\"\n",
    "        quality_map = {\n",
    "            \"journal\": 3,\n",
    "            \"conference\": 2,\n",
    "            \"arxiv\": 1,\n",
    "            \"other\": 0,\n",
    "        }\n",
    "        scores = []\n",
    "        for cid in used_ids:\n",
    "            ref = next((r for r in self.reference_db if r.id == cid), None)\n",
    "            if ref:\n",
    "                scores.append(quality_map.get(ref.source_type.lower(), 0))\n",
    "        if not scores:\n",
    "            return 0.0\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    def _compute_recency_score(self, used_ids: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Higher is 'more recent on average'.\n",
    "        We convert publication year into a normalized score [0,1] over the observed range in reference_db.\n",
    "        \"\"\"\n",
    "        years_all = [r.year for r in self.reference_db if r.year is not None]\n",
    "        if not years_all:\n",
    "            return 0.0\n",
    "        min_year, max_year = min(years_all), max(years_all)\n",
    "        if min_year == max_year:\n",
    "            return 1.0\n",
    "\n",
    "        years_used = []\n",
    "        for cid in used_ids:\n",
    "            ref = next((r for r in self.reference_db if r.id == cid and r.year is not None), None)\n",
    "            if ref:\n",
    "                years_used.append(ref.year)\n",
    "\n",
    "        if not years_used:\n",
    "            return 0.0\n",
    "\n",
    "        # Normalize: (year - min_year) / (max_year - min_year)\n",
    "        norm_scores = [(y - min_year) / (max_year - min_year) for y in years_used]\n",
    "        return sum(norm_scores) / len(norm_scores)\n",
    "\n",
    "    def evaluate_references(self, answer: str) -> ReferenceScores:\n",
    "        used_cids = self._extract_citation_ids(answer)\n",
    "\n",
    "        hallucinated = 0\n",
    "        for cid in used_cids:\n",
    "            if cid not in self.reference_ids:\n",
    "                hallucinated += 1\n",
    "\n",
    "        source_quality = self._compute_source_quality_score(used_cids)\n",
    "        recency_score = self._compute_recency_score(used_cids)\n",
    "\n",
    "        return ReferenceScores(\n",
    "            hallucinated_citations_per_answer=hallucinated,\n",
    "            source_quality_score=source_quality,\n",
    "            recency_score=recency_score,\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Full rubric evaluation\n",
    "    # -------------------------\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "    ) -> RubricResult:\n",
    "        \"\"\"\n",
    "        Main entry point.\n",
    "        \"\"\"\n",
    "        ling = self.evaluate_linguistic_clarity(answer)\n",
    "        sci = self.evaluate_scientific_accuracy(question, answer)\n",
    "        refs = self.evaluate_references(answer)\n",
    "        return RubricResult(\n",
    "            linguistic_clarity=ling,\n",
    "            scientific_accuracy=sci,\n",
    "            references=refs,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BASE_DIR: /Users/swetangkrishna/Desktop/monolingual_llms\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RQ1_QUESTION = (\n",
    "    \"How does lexical gap between languages impact the reasoning abilities of models?\"\n",
    ")\n",
    "\n",
    "RQ2_QUESTION = (\n",
    "    \"Do monolingual models perform differently on reasoning tasks depending on the \"\n",
    "    \"language in which they are trained? And does it affect their efficiency?\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. File mapping: which model answer is in which file\n",
    "# --------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path.cwd()  # change this to Path.cwd() / \"MONOLINGUAL_LLMS\" if needed\n",
    "\n",
    "print(\"Using BASE_DIR:\", BASE_DIR)\n",
    "\n",
    "ANSWER_FILES = {\n",
    "    \"RQ1\": {\n",
    "        \"gpt-5.1\": \"gpt-5.1(RQ1).txt\",\n",
    "        \"deepseek-R1\": \"deepseek-R1(RQ1).txt\",\n",
    "    },\n",
    "    \"RQ2\": {\n",
    "        \"gpt-5.1\": \"gpt-5.1(RQ2).txt\",\n",
    "        \"deepseek-R1\": \"deepseek-R1(RQ2).txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "RQ_TEXT = {\n",
    "    \"RQ1\": RQ1_QUESTION,\n",
    "    \"RQ2\": RQ2_QUESTION,\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Instantiate evaluator for this project context\n",
    "#    (assumes JARGON_LIST, CONNECTORS, reference_db, RubricEvaluator already defined)\n",
    "# --------------------------------------------------\n",
    "\n",
    "evaluator = RubricEvaluator(\n",
    "    jargon_list=JARGON_LIST,\n",
    "    connectors=CONNECTORS,\n",
    "    #reference_db=reference_db,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Helper to safely read text files\n",
    "# --------------------------------------------------\n",
    "\n",
    "def read_answer_file(relative_path: str) -> str:\n",
    "    full_path = BASE_DIR / relative_path\n",
    "    print(f\"Reading: {full_path}\")  # debug print\n",
    "    if not full_path.exists():\n",
    "        raise FileNotFoundError(f\"Answer file not found: {full_path}\")\n",
    "    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Run evaluation\n",
    "# --------------------------------------------------\n",
    "\n",
    "def evaluate_all_answers():\n",
    "    results = {}\n",
    "\n",
    "    for rq_id, models in ANSWER_FILES.items():\n",
    "        question_text = RQ_TEXT[rq_id]\n",
    "        results[rq_id] = {}\n",
    "\n",
    "        for model_name, rel_path in models.items():\n",
    "            answer_text = read_answer_file(rel_path)\n",
    "\n",
    "            rubric_result = evaluator.evaluate(\n",
    "                question=question_text,\n",
    "                answer=answer_text,\n",
    "            )\n",
    "\n",
    "            results[rq_id][model_name] = rubric_result.as_dict()\n",
    "\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"{rq_id} – {model_name}\")\n",
    "            print(f\"Question: {question_text}\")\n",
    "            print(f\"File: {rel_path}\")\n",
    "            print(\"- Rubric scores:\")\n",
    "            print(json.dumps(rubric_result.as_dict(), indent=2))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/swetangkrishna/Desktop/monolingual_llms/gpt-5.1(RQ1).txt\n",
      "======================================================================\n",
      "RQ1 – gpt-5.1\n",
      "Question: How does lexical gap between languages impact the reasoning abilities of models?\n",
      "File: gpt-5.1(RQ1).txt\n",
      "- Rubric scores:\n",
      "{\n",
      "  \"linguistic_clarity\": {\n",
      "    \"words_per_sentence\": 9.987804878048781,\n",
      "    \"jargon_per_sentence\": 0.2926829268292683,\n",
      "    \"flesch_reading_ease\": 28.478696730099188,\n",
      "    \"connectors_per_sentence\": 0.39634146341463417\n",
      "  },\n",
      "  \"scientific_accuracy\": {\n",
      "    \"quasi_definitions_per_answer\": 0,\n",
      "    \"bias_markers_per_answer\": 0,\n",
      "    \"cosine_similarity_q_a\": 0.2951676132706148\n",
      "  },\n",
      "  \"references\": {\n",
      "    \"hallucinated_citations_per_answer\": 0,\n",
      "    \"source_quality_score\": 0.0,\n",
      "    \"recency_score\": 0.0\n",
      "  }\n",
      "}\n",
      "Reading: /Users/swetangkrishna/Desktop/monolingual_llms/deepseek-R1(RQ1).txt\n",
      "======================================================================\n",
      "RQ1 – deepseek-R1\n",
      "Question: How does lexical gap between languages impact the reasoning abilities of models?\n",
      "File: deepseek-R1(RQ1).txt\n",
      "- Rubric scores:\n",
      "{\n",
      "  \"linguistic_clarity\": {\n",
      "    \"words_per_sentence\": 11.317073170731707,\n",
      "    \"jargon_per_sentence\": 0.056910569105691054,\n",
      "    \"flesch_reading_ease\": 26.148170731707353,\n",
      "    \"connectors_per_sentence\": 0.4146341463414634\n",
      "  },\n",
      "  \"scientific_accuracy\": {\n",
      "    \"quasi_definitions_per_answer\": 0,\n",
      "    \"bias_markers_per_answer\": 0,\n",
      "    \"cosine_similarity_q_a\": 0.3529659893218757\n",
      "  },\n",
      "  \"references\": {\n",
      "    \"hallucinated_citations_per_answer\": 0,\n",
      "    \"source_quality_score\": 0.0,\n",
      "    \"recency_score\": 0.0\n",
      "  }\n",
      "}\n",
      "Reading: /Users/swetangkrishna/Desktop/monolingual_llms/gpt-5.1(RQ2).txt\n",
      "======================================================================\n",
      "RQ2 – gpt-5.1\n",
      "Question: Do monolingual models perform differently on reasoning tasks depending on the language in which they are trained? And does it affect their efficiency?\n",
      "File: gpt-5.1(RQ2).txt\n",
      "- Rubric scores:\n",
      "{\n",
      "  \"linguistic_clarity\": {\n",
      "    \"words_per_sentence\": 13.086538461538462,\n",
      "    \"jargon_per_sentence\": 0.5576923076923077,\n",
      "    \"flesch_reading_ease\": 27.398012102243825,\n",
      "    \"connectors_per_sentence\": 0.6730769230769231\n",
      "  },\n",
      "  \"scientific_accuracy\": {\n",
      "    \"quasi_definitions_per_answer\": 0,\n",
      "    \"bias_markers_per_answer\": 0,\n",
      "    \"cosine_similarity_q_a\": 0.37808917631809513\n",
      "  },\n",
      "  \"references\": {\n",
      "    \"hallucinated_citations_per_answer\": 0,\n",
      "    \"source_quality_score\": 0.0,\n",
      "    \"recency_score\": 0.0\n",
      "  }\n",
      "}\n",
      "Reading: /Users/swetangkrishna/Desktop/monolingual_llms/deepseek-R1(RQ2).txt\n",
      "======================================================================\n",
      "RQ2 – deepseek-R1\n",
      "Question: Do monolingual models perform differently on reasoning tasks depending on the language in which they are trained? And does it affect their efficiency?\n",
      "File: deepseek-R1(RQ2).txt\n",
      "- Rubric scores:\n",
      "{\n",
      "  \"linguistic_clarity\": {\n",
      "    \"words_per_sentence\": 13.601307189542483,\n",
      "    \"jargon_per_sentence\": 0.5032679738562091,\n",
      "    \"flesch_reading_ease\": 17.203147493820524,\n",
      "    \"connectors_per_sentence\": 0.3333333333333333\n",
      "  },\n",
      "  \"scientific_accuracy\": {\n",
      "    \"quasi_definitions_per_answer\": 0,\n",
      "    \"bias_markers_per_answer\": 0,\n",
      "    \"cosine_similarity_q_a\": 0.3779478352285338\n",
      "  },\n",
      "  \"references\": {\n",
      "    \"hallucinated_citations_per_answer\": 0,\n",
      "    \"source_quality_score\": 0.0,\n",
      "    \"recency_score\": 0.0\n",
      "  }\n",
      "}\n",
      "\n",
      "Saved all rubric results to /Users/swetangkrishna/Desktop/monolingual_llms/rubric_results_RQ1_RQ2.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_results = evaluate_all_answers()\n",
    "    with open(BASE_DIR / \"rubric_results_RQ1_RQ2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    print(\"\\nSaved all rubric results to\", BASE_DIR / \"rubric_results_RQ1_RQ2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SQL_tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
