# Rubric-Based Evaluation System for Research Question Answers

## ğŸ“˜ Overview

This project provides an automated **rubric-based evaluation framework** for assessing model-generated answers to two predefined research questions (RQ1 and RQ2).  
The system evaluates each answer along three major dimensions:

1. **Linguistic Clarity** â€“ How readable, clear, and coherent the answer is.
2. **Scientific Accuracy** â€“ Whether the answer is relevant, rigorous, unbiased, and well structured.
3. **Reference Quality** â€“ What citations are used and how credible they are (optional, based on metadata).

The evaluation results are printed to console and also saved into a structured JSON file for further analysis.

---

## ğŸ§  Research Questions

The evaluation focuses on analyzing model responses to:

### **RQ1**  
**How does lexical gap between languages impact the reasoning abilities of models?**

### **RQ2**  
**Do monolingual models perform differently on reasoning tasks across multiple languages?  
Does this performance difference affect their efficiency?**

Each model produces a text answer for each question. These answers are evaluated independently through the rubric.

---

## ğŸ“‚ Input Structure

The evaluation script expects answer files in the following format:

```
project/
â”‚
â”œâ”€â”€ gpt-5.1(RQ1).txt
â”œâ”€â”€ deepseek-R1(RQ1).txt
â”œâ”€â”€ gpt-5.1(RQ2).txt
â””â”€â”€ deepseek-R1(RQ2).txt
```

Each file contains the full textual answer from one model.

---

## âš™ï¸ Evaluation Pipeline (Step-by-Step)

The entire evaluation is orchestrated through the `RubricEvaluator` class, which contains logic for clarity scoring, scientific scoring, and citation scoring.

### ### 1ï¸âƒ£ Input Processing

1. The system loads the answer file (e.g., `gpt-5.1(RQ1).txt`).
2. The corresponding research question text is fetched.
3. The answer is passed into the evaluation pipeline.

---

## âœï¸ 2ï¸âƒ£ Linguistic Clarity Scoring

The evaluator checks:

### **âœ“ Sentence Length**
- Splits the answer into sentences.
- Computes average words per sentence.

### **âœ“ Jargon Usage**
- Matches words against a predefined list of domain-specific jargon terms.
- Computes jargon density per sentence.

### **âœ“ Readability (Flesch Reading Ease Score)**
- Uses a custom syllable counter.
- Computes readability using the Flesch formula.

### **âœ“ Cohesion via Connectors**
- Counts usages of logical connectors such as *however*, *therefore*, *moreover*, etc.
- Measures the logical flow of the response.

All these metrics are wrapped into a `LinguisticClarityScores` dataclass.

---

## ğŸ”¬ 3ï¸âƒ£ Scientific Accuracy Scoring

This section evaluates whether the answer is **scientifically grounded**, **relevant**, and **unbiased**.

### **âœ“ Quasi-Definitions**
Checks for phrases like:
- *â€œis defined asâ€*
- *â€œrefers toâ€*
- *â€œis understood asâ€*

More definitions â†’ stronger conceptual clarity.

### **âœ“ Bias Marker Detection**
Detects subjective or emotionally loaded words like:
- *clearly*, *obviously*, *always*, *never*, *perfect*, etc.

If such terms appear in both the question and answer â†’ potential bias reinforcement.

### **âœ“ Cosine Similarity**
Uses TFâ€“IDF vectorization to measure:
**How closely the answer content aligns with the question.**

Produces a score between **0.0** and **1.0**.

All results are stored in `ScientificAccuracyScores`.

---

## ğŸ“š 4ï¸âƒ£ Reference Evaluation (Optional)

If citations appear inside the answer, the system extracts them using:

- **Numeric citation pattern:** `[1]`, `[2]`, etc.
- **Author-year pattern:** `(Smith, 2020)`

If a **reference metadata database** is supplied, the evaluator also computes:

- **Citation coverage**
- **Citation quality (journal > conference > preprint)**
- **Citation recency**

Since no metadata is currently enabled, these scores default to **0**.

---

## ğŸ 5ï¸âƒ£ Final Output

After evaluating all models for both research questions, the system saves results to:

```
rubric_results_RQ1_RQ2.json
```

This JSON file contains:

```json
{
  "RQ1": {
    "gpt-5.1": { ... linguistic, scientific, and reference scores ... },
    "deepseek-R1": { ... }
  },
  "RQ2": {
    "gpt-5.1": { ... },
    "deepseek-R1": { ... }
  }
}
```

---

## ğŸ”§ How to Run

Run the Jupyter notebook or Python script containing:

```python
if __name__ == "__main__":
    all_results = evaluate_all_answers()
```

This will:

- Load all answer files
- Evaluate each answer systematically
- Print results
- Save JSON output

---

## ğŸ“¦ Project Structure

```
project/
â”‚
â”œâ”€â”€ prac.ipynb
â”œâ”€â”€ gpt-5.1(RQ1).txt
â”œâ”€â”€ deepseek-R1(RQ1).txt
â”œâ”€â”€ gpt-5.1(RQ2).txt
â”œâ”€â”€ deepseek-R1(RQ2).txt
â””â”€â”€ rubric_results_RQ1_RQ2.json  â† auto-generated output
```

---

## ğŸ§© Extensibility

You can easily extend:

- **The jargon list** for domain tuning  
- **Connector list** for style analysis  
- **Reference metadata** for real citation scoring  
- **Bias marker lists** for deeper fairness analysis  
- **Cosine similarity** to include embeddings (e.g., sentence transformers)

---

## ğŸ“ Summary

This project provides a fully automated, reproducible, and extensible evaluation pipeline for comparing LLM responses across:

- **Readability**
- **Scientific depth**
- **Relevance**
- **Citation behavior**

It is especially useful for research comparing LLM model families or multilingual reasoning behavior.

---

If you'd like, I can also generate:

- A version with diagrams  
- A GitHub-friendly short README  
- A PDF documentation file  

Just ask!
